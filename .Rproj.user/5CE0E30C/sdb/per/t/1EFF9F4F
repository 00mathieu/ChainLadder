{
    "contents" : "\\documentclass[ChapterTOCs, krantz1]{CAS}\n\\usepackage{amssymb}\n\\usepackage{amsmath}\n\\usepackage{graphicx}\n%\\usepackage{subfigure}\n%\\usepackage{makeidx}\n\\usepackage{multicol}\n\\usepackage{verbatim}\n\\usepackage{url}\n%\\usepackage{longtable}\n%\\usepackage{colortbl}\n%\\usepackage{supertabular}\n\\usepackage{natbib}\n\\usepackage{booktabs}\n\\usepackage{listings}\n\\usepackage{fancyhdr, fancyvrb}\n%\\usepackage{upgreek}\n%\\usepackage{natbib}\n%\\usepackage{amsmath,lineno,natbib,paralist,bm,graphicx,setspace,\n%            booktabs,url,todonotes}\n\\usepackage{geometry}\n%\\bibliographystyle{agsm}\n%\\usepackage{xcolor}\n\\urlstyle{sf}\n\\graphicspath{{figures/}}\n\n\\setcounter{topnumber}{2}\n\\setcounter{bottomnumber}{2}\n\\setcounter{totalnumber}{4}\n\\renewcommand{\\topfraction}{0.85}\n\\renewcommand{\\bottomfraction}{0.85}\n\\renewcommand{\\textfraction}{0.15}\n\\renewcommand{\\floatpagefraction}{0.7}\n\n\\usepackage{Sweave} %added by Giorgio Spedicato 26-12-2012\n\\SweaveOpts{prefix.string=figures/fig}\n%\\usepackage{lifecon} %added by Giorgio Spedicato 26-12-2012\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\n\\frenchspacing\n\\tolerance=5000\n\n\\include{list-command}\n\\include{list-preamble}\n\n%\\include{Chapters/chapter1/preamble}\n%\\include{Chapters/chapter8/preamble}\n\\include{preamble}\n\\makeatletter\n\\makeatother\n\n\\makeindex\n\n\\begin{document}\n\n\\title{Computational Actuarial Science, with \\R}\n\n\\author{\\today}\n\n\\maketitle\n\n\\frontmatter\n%\\include{frontmatter/Foreword}\n%\\include{frontmatter/preface}\n%\\include{contributor}\n\n%\\listoffigures\n%\\listoftables\n\\tableofcontents\n\n\\mainmatter\n\n%\\include{Chapters/symbollist}\n\n%%%%%%%%%%%%%%%%%%%%%%\n%%% Markus Gesmann %%%\n%%%%%%%%%%%%%%%%%%%%%%\n\n\\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}\n<<options, echo=FALSE>>=\noptions(prompt = \"R> \", digits = 4, show.signif.stars = TRUE)\noptions(continue=\"   \")\nlibrary(ChainLadder)\nlibrary(lattice)\nlibrary(AER)\nlibrary(fitdistrplus)\nlattice.options(default.theme = standard.theme(color = FALSE))\n@\n\\SweaveOpts{concordance=TRUE}\n\n\n\\newcommand{\\chainladder}{\\textbf{\\texttt{ChainLadder}} }\n\\chapterauthor{Markus Gesmann}{ChainLadder project}\n\\chapter{Claims reserving and IBNR}\n\n\\section{Introduction}\n\\subsection{Motivation}\n\nThe insurance industry, unlike other industries, does not sell products\nas such but promises. An insurance policy is a promise by the insurer\nto the policyholder to pay for future claims for an upfront received\npremium. \n\nAs a result insurers don't know the upfront cost for their\nservice, but rely on historical data analysis and judgement to predict a\nsustainable price for their offering. In General Insurance (or Non-Life\nInsurance, e.g. motor, property and casualty insurance) most policies\nrun for a period of 12 months. However, the claims payment process \ncan take years or even decades. Therefore often not even the delivery\ndate of their product is known to insurers. \n\nIn particular losses arising from casualty insurance can take a long\ntime to settle and even when the claims are acknowledged it may take \ntime to establish the extent of the claims settlement cost. \nClaims can take years to materialize. A complex and\ncostly example are the claims from asbestos liabilities, \nparticularly those in connection with mesothelioma and lung damage \narising from prolonged exposure to asbestos. \nA research report by a  working party of the Institute and Faculty of Actuaries\nestimated that the un-discounted cost of UK mesothelioma-related\nclaims to the UK Insurance Market for the period 2009 to 2050 could be\naround $\\pounds$10bn, see \\cite{Gravelsons2009}. The cost for asbestos\nrelated claims in the US for the worldwide insurance industry was\nestimate to be around \\$120bn in 2002, see \\cite{Michaels2002}.  \n\nThus, it should come as no surprise that the biggest item on the\nliabilities side of an insurer's balance sheet is often the provision or\nreserves for future claims payments. Those reserves can be broken\ndown in case reserves (or outstanding claims), which are losses\nalready reported to the insurance company and losses that are incurred \nbut not reported (IBNR) yet.\n\nHistorically, reserving was based on deterministic\ncalculations with pen and paper, combined with expert\njudgement. Since the 1980's, with the arrival of personal computer,\nspreadsheet software became very popular for reserving. \nSpreadsheets not only reduced the calculation time, but allowed actuaries\nto test different scenarios and the sensitivity of their\nforecasts.\n\nAs the computer became more powerful, ideas of more sophisticated models \nstarted to evolve. Changes in regulatory requirements, e.g. \nSolvency II\\footnote{See\n  \\url{http://ec.europa.eu/internal_market/insurance/solvency/index_en.htm}}\nin Europe, have fostered further research and promoted the use of stochastic and\nstatistical techniques. In particular, for many countries extreme \npercentiles of reserve deterioration over a fixed time period have to be \nestimated for the purpose of capital setting.  \n\nOver the years several methods and models have been developed to\nestimate both the level and variability of reserves for insurance claims, \nsee~\\cite{Schmidt2011} or\n\\cite{EnglandVerrall2002} for an overview. \n\nIn practice the Mack chain-ladder and bootstrap chain-ladder models are used\nby many actuaries along with stress testing / scenario analysis and expert \njudgement to estimate ranges of reasonable outcomes, see the surveys of UK \nactuaries in 2002,~\\cite{ClaimsReservingWorkingParty:2002}, and\nacross the Lloyd's market in 2012,~\\cite{JamesOrr:2012}. \n\n\\subsection{Outline and scope}\nIn this chapter we can only give an introduction to some reserving\nmodels and the focus will be on the practical \nimplementation in \\textsf{R}. For a more comprehensive overview see\n\\cite{WuetherichMerz:2008}.  The remainder of this chapter is structured as \nfollows. Section~\\ref{sec:triangles} gives an overview to the data structure\nused for a typical reserving exercise and introduces the example data set used \nthroughout this chapter. We discuss the classical deterministic \nchain-ladder reserving method in section~\\ref{sec:deterministic}\nand introduce the concept of a tail factor.\nIn section~\\ref{sec:stochasticreserving} we show first that the chain-ladder\nalgorithm can be considered as a weighted linear regression through \nthe origin and move on from there to introduce stochastic reserving models.\nWe start with the Mack model, which provides a stochastic framework for \nthe chain-ladder methods and allows the estimation of the \nmean squared error of the payment predictions. Following this we discuss the\nPoisson model, a generalised linear model that replicates the \nchain-ladder forecasts. To estimate the full distribution of the \nreserve we consider a bootstrap approach. We finish the section with\na log-incremental reserving model that is particular suited to identify \nchanging trends in the data. \nFinally, section~\\ref{sec:reserverisk} will briefly discuss \nthe differences between the ultimo and one year reserve risk measurements \nin the context of Solvency~II.\n\n%Here we will discuss the basic chain-ladder model, the Mack\n%model (see \\cite{Mack1993,Mack1999}) and bootstrap model as presented by\n%\\cite{EnglandVerrall2002}, AND, AND \n\n\\section{Development triangles}\\label{sec:triangles}\n\nHistorical insurance data is often presented in form of a triangle\nstructure, showing the development of claims over time for each\nexposure (origin) period. An origin period could be the year the\npolicy was written or earned, or the loss occurrence period. Of course the\norigin period doesn't have to be yearly, e.g. \nquarterly or monthly origin periods are also often used. \nThe development period of an origin period is also called age or lag.\nData on the diagonals present payments in the same calendar period.\nNote, data of individual policies is usually aggregated to homogeneous \nlines of business, division levels or perils.\n\nAs an example we present a claims payment triangle from a UK Motor\nNon-Comprehensive account as published by~\\cite{Christofides1997}. For\nconvenience we set the origin period from 2007 to 2013.\n\nThe following data frame presents the claims data in a typical form as\nit would be stored in a data base. The first column holds the origin\nyear, the second column the development year and the third\ncolumn has the incremental payments / transactions.\n\n<<claimsdata>>=\nn <- 7\nClaims <- \n  data.frame(originf = factor(rep(2007:2013, n:1)),\n             dev=sequence(n:1),\n             inc.paid= \n             c(3511, 3215, 2266, 1712, 1059, 587, \n               340, 4001, 3702, 2278, 1180,  956,\n               629, 4355, 3932, 1946, 1522, 1238, \n               4295, 3455, 2023, 1320, 4150, 3747, \n               2320, 5102, 4548, 6283))\n@ \nTo present the data in a triangle format we can use the \\texttt{matrix}\nfunction: % of the \\texttt{reshape2} package:\n%% library(reshape2)\n%% (inc.triangle  <- acast(Claims, origin ~ dev , value.var='inc.paid'))\n<<triangle>>=\n(inc.triangle  <- with(Claims, {\n  M <- matrix(nrow=n, ncol=n, \n              dimnames=list(origin=levels(originf), dev=1:n))\n  M[cbind(originf, dev)] <- inc.paid\n  M\n}))\n@ \nIt is the objective of a reserving exercise to forecast the future claims\ndevelopment in the bottom right corner of the triangle and potential\nfurther developments beyond development age 7. Eventually all claims\nfor a given origin period will be settled, but it is not always\nobvious to judge how many years or even decades it will take. We speak\nof long and short tail business depending on the time it takes to pay\nall claims. \n\nOften it is helpful to consider the cumulative development of claims\nas well, which is presented below.  \n<<cumtriangle>>=\n(cum.triangle <- t(apply(inc.triangle, 1, cumsum)))\n@\nThe latest diagonal of the triangle presents the latest cumulative paid position\nof all origin years:\n<<latestpaid>>=\n(latest.paid <- cum.triangle[row(cum.triangle) == n - col(cum.triangle) + 1])\n@\nWe add the cumulative paid data as column to the data frame as well.\n<<>>=\nClaims$cum.paid <- cum.triangle[with(Claims, cbind(originf, dev))]\n@\nTo start the reserving analysis we plot the data.\n<<label=triangleplotinteraction, include=FALSE>>=\nop <- par(fig=c(0,0.5,0,1), cex=0.8, oma=c(0,0,0,0))\nwith(Claims, {\n  interaction.plot(x.factor=dev, trace.factor=originf, response=inc.paid, \n                   fun=sum, type=\"b\", bty='n', legend=FALSE); axis(1, at=1:n)\n  par(fig=c(0.45,1,0,1), new=TRUE, cex=0.8, oma=c(0,0,0,0))\n  interaction.plot(x.factor=dev, trace.factor=originf, response=cum.paid, \n                   fun=sum, type=\"b\", bty='n'); axis(1,at=1:n)\n})\nmtext(\"Incremental and cumulative claims development\", \n      side=3, outer=TRUE, line=-3, cex = 1.1, font=2)\npar(op)\n@\n\\begin{figure}[thb]\n  \\begin{center}\n  \\setkeys{Gin}{width=0.75\\textwidth}\n<<triangleplotinteractionplot, fig=TRUE, echo=FALSE, height=4>>=\n<<triangleplotinteraction>>\n@\n\\caption{Plot of incremental and cumulative claims payments by origin\n  year using base graphics, using \\texttt{interaction.plot} of the \n  \\texttt{stats} package in \\textsf{R}.}\n\\label{fig:triangleinteraction}\n\\end{center}\n\\end{figure}\n\n\n<<label=trianglelattice, include=FALSE>>=\nlibrary(lattice)\nxyplot(cum.paid ~ dev | originf, data=Claims, t=\"b\", layout=c(4,2),\n       as.table=TRUE, main=\"Cumulative claims development\")\n@\n\\begin{figure}[thb]\n  \\begin{center}\n  \\setkeys{Gin}{width=0.75\\textwidth}\n<<triangleplotlattice, fig=TRUE, echo=FALSE, height=4.5>>=\n<<trianglelattice>>\n@ \n\\caption{Claims developments by origin year using the\n  \\texttt{lattice} package, with one panel per origin year.}\n\\label{fig:trianglelattice}\n\\end{center}\n\\end{figure}\nFigures~\\ref{fig:triangleinteraction} and~\\ref{fig:trianglelattice}\npresent the incremental and cumulative claims development by origin\nyear. The triangle appears to be fairly well behaved. The last two years, \n2012 and 2013 appear to be slightly higher than years 2008 to 2011  and the\nvalues in 2007 are lower in comparison to the later years, e.g. \nthe book changed over the years. The last\npayment of 1,238 for the 2009 origin year stands out a bit as well. \n\nOther claims information can provide valuable insight into the\nreserving process too, such as claims numbers, transition timings between \ndifferent claims settlement stages and earning patterns. See for\nexample~\\cite{MariaDoloresMartinezMiranda2011, Orr2007, Murray2011} \nrespectively. A deep understanding of the whole business process from pricing,\nunderwriting, claims handling and data management will guide the\nactuary to interpret the claims data at hand. \nThe Claims Reserving Working Party Paper, \\cite{ClaimsReservingWorkingParty:2002},\noutlines the different aspects in more detail.\n\n\n\\paragraph{Notation}\n\nUsing terminology in \\cite{WuetherichMerz:2008}, cumulative\npayments are noted as $C_{i,j}$, for origin period $i$ (or period of\noccurrence) seen after $j$ periods of development and incremental\npayments $X_{i,j}$. \n%Past observations, cumulative and incremental,\n%are abbreviated as $\\mathcal{F}$ \\textbf{CORRECT??}.\n%%% arthur\nThe outstanding liabilities, or reserves, for accident year $i$ at time $j$ is given by\n\\begin{align}\nR_{i,j} = \\sum_{k > j} X_{i,j} = \\left( \\lim_{k\\rightarrow\\infty} C_{i,k}\\right)-C_{i,j}\n\\label{eq:reserves}\n\\end{align}\nSince this quantity involves unobserved data (i.e. amounts that will be paid\nin the future), $R_{i,j}$ will the estimated claims reserves. \n%% given past observations in the  triangle, denoted $\\mathcal{F}$,\n\\begin{remark}\nFor convenience, we will assume that we work on square matrices,\nwith $n$ rows, and $n$ columns. For a more general setting, see\n\\cite{WuetherichMerz:2008}. \n\\end{remark}\n\n\\begin{remark}\nThroughout this chapter we note the first development period as 1. \nOther authors use 0 for the first development period. This is more\na matter of taste, than having any practical implications.\n\\end{remark}\n\n\\begin{remark}\nMany of the methods and models presented here can be applied to paid \nand reported (often also called 'incurred') data. We either have to estimate \nthe reserve or incurred but not reported (IBNR) claims.\nFor the purpose of this chapter we assume:\n\\begin{align}\n\\mbox{Ultimate loss cost} & = \\mbox{paid} + \\mbox{reserve} \\\\\n    & = \\mbox{paid} + \\mbox{case reserve}  + \\mbox{IBNR} \\\\\n    & = \\mbox{incurred} + \\mbox{IBNR}\n\\end{align}\n\\end{remark}\n\n\\begin{remark}\nSome methods and models require data $>0$, which for paid claims should \nbe given (insurers rarely receive money back from the insured after a \nclaim was paid\\footnote{Examples are late salvage or subrogation payments.}), \nbut case reserves can show negative adjustments over \ntime, therefore incremental incurred triangles do show negatives \noccasionally. To ensure that data has only positive values it can be \ntemporarily shifted, or, in a given context, be ignored.\n\\end{remark}\n\n\n\\section{Deterministic reserving methods}\\label{sec:deterministic}\n\nThe most established and probably oldest method or algorithm for\nestimating reserves is the so called chain-ladder method or loss\ndevelopment factor (LDF) method.\n\nThe classical chain-ladder method is a  deterministic algorithm \nto forecast claims based on historical data. It assumes that the\nproportional developments of claims from one development period to the\nnext is the same for all origin periods.\n\n\\subsection{Chain-ladder algorithm}\n\nMost commonly as a first step, the age-to-age link ratios $f_k$ are\ncalculated as the volume weighted average development ratios of a\ncumulative loss development triangle from one age \nperiod to the next $C_{ik}$ for $i,k=1, \\dots, n$.  \n\n\\begin{align}\n  f_{k} &= \\frac{\\sum_{i=1}^{n-k} C_{i,k+1}}{\\sum_{i=1}^{n-k}C_{i,k}}\n\\end{align}\n\n<<>>=\nf <- sapply((n-1):1, function(i) { \n  sum( cum.triangle[1:i, n-i+1] ) / sum( cum.triangle[1:i, n-i] )\n})\n@\nInitially we expect no further development after year 7.  Hence, we set the \nlast link ratio (often called the tail factor) to 1.\n<<>>=\ntail <- 1\n(f <- c(f, tail))\n@ \nThese factors $f_k$ are then applied to the latest cumulative payment in\neach row ($C_{i, n-i+1})$ to produce stepwise forecasts for future payment\nyears $k \\in \\{n-i+ 1, \\dots, n\\}$:\n\\begin{align}\n \\hat{C}_{i,k + 1} &  =f_{k} \\hat{C}_{i, k}\n\\end{align}\nstarting with $\\hat{C}_{i, n+1-i} = C_{i, n+1-i}$.\nThe \\emph{squaring} of the claims triangle is calculated below. \n<<>>=\nfull.triangle <- cum.triangle\nfor(k in 1:(n-1)){ \n  full.triangle[(n-k+1):n, k+1] <- full.triangle[(n-k+1):n,k]*f[k]\n}\nfull.triangle\n@ \nThe last column contains the forecast ultimate loss cost.\n<<ultimatepaid>>=\n(ultimate.paid <- full.triangle[,n])\n@\nThe cumulative products of the age-to-age development ratios provide\nthe loss development factors for the latest cumulative paid claims for\neach row to ultimate. \n<<LDFs>>=\n(ldf <- rev(cumprod(rev(f))))\n@ \nThe inverse of the loss development factor\nestimates the proportion of claims developed to date for each origin year,\noften also called the gross up factors or growth curve.\n<<>>=\n(dev.pattern <- 1/ldf)\n@\nThe total estimated outstanding loss reserve with this method is: \n%##latestData <- Claims[, .SD[max(dev)] , by=list(origin)]\n%##sum(full.triangle[, n] - latest.paid)\n<<>>=\n(reserve <- sum (latest.paid * (ldf - 1)))\n@\nor via\n<<>>=\nsum(ultimate.paid - latest.paid)\n@ \n\n\\begin{remark}\\label{re:clmultiplicative}\nThe basic chain-ladder algorithm has the implicit assumption that each origin \nperiod has its own unique level and that development factors are independent \nof the origin periods, or equivalently, there is a constant payment pattern.\nTherefore, if $a_i$ is the ultimate (cumulative) claim for origin period $i$ \nand $b_j$ is the percentage of ultimate claims in development period $j$, \nwith $\\sum b_j = 1$, then the incremental payment $\\hat{X}_{ij}$ can be described as \n$\\hat{X}_{ij} = a_i b_j$, see~\\cite{Christofides1997}.\n\\end{remark}\n<<>>=\na <- ultimate.paid\n(b <- c(dev.pattern[1], diff(dev.pattern)))\n(X.hat <- a %*% t(b))\n@\n% \\begin{exercise}\n% The residuals of the chain-ladder method given as\n% <<>>=\n% res <- inc.triangle - X.hat\n% @\n% What information can we extract?\n% \\end{exercise}\n\\begin{remark}\nAs the chain-ladder method is a deterministic algorithm and doesn't regard the \nobservations as realisations of random variables but absolute values, the \nforecast of the most recent origin periods can be quite unstable. \nTo address this issue~\\cite{BornhuetterFerguson1972} suggested a credibility \napproach, which combines the chain-ladder forecast with prior information on \nexpected loss costs, e.g. from pricing data. Under this approach the chain-ladder \ndevelopment to ultimate pattern  is used as weighting factors between the pure \nchain-ladder and expected loss cost estimates.\n\nSuppose the expected loss cost for the 2013 origin year is 20,000, then\nthe BF method would estimate the ultimate loss cost as:\n\\end{remark}\n<<BF>>=\n(BF2013 <- ultimate.paid[n] * dev.pattern[1] + 20000 * (1 - dev.pattern[1]))\n@\n\n\n\\subsection{Tail factors}\n\nIn the previous section we implicitly assumed that there are no claims\npayment after 7 years, or in other words that the oldest origin\nyear is fully developed. \n\nHowever, often it is not suitable to assume that the oldest origin year is\nfully settled. A typical approach to overcome this shortcoming is to \nextrapolate the development ratios, e.g. assuming a linear model of \nthe log development ratios minus one, which reflects the incremental changes on \nthe previous cumulative payments, see also Figure~\\ref{fig:tail}. \n<<>>=\ndat <- data.frame(lf1=log(f[-c(1,n)]-1), dev=2:(n-1))\n(m <- lm(lf1 ~ dev , data=dat))\n@\n<<label=tailf, include=FALSE>>=\nplot(lf1 ~ dev, main=\"log(f - 1) ~ dev\", data=dat, bty='n')\nabline(m)\n@ \n<<>>=\nsigma <- summary(m)$sigma\nextrapolation <- predict(m, data.frame(dev=n:100))\n(tail <- prod(exp(extrapolation + 0.5*sigma^2) + 1))\n@ \nWe haven't carried out any sense checks apart \nfrom the plot in Figure~\\ref{fig:tail}; however, the ratio analysis presented \nabove would suggest that we can expect \nanother 3.7\\% claims development after year 7 and therefore we should \nconsider increasing our reserve to 29,728.\n\n\\begin{figure}[thb]\n  \\begin{center}\n  \\setkeys{Gin}{width=0.5\\textwidth}\n<<tailfplot, fig=TRUE, echo=FALSE, width=4, height=4>>=\n<<tailf>>\n@\n\\caption{Plot of the loss development factors - 1 on a log-scale\n  against development period}\n\\label{fig:tail}\n\\end{center}\n\\end{figure}\n%\\setkeys{Gin}{width=0.8\\textwidth}\n\n%%%%%%%%%%%%%%%%%\n\nMore generally, the factors used to project the future payments need not\nalways be drawn from the dollar weighted averages of the\ntriangle. Other sources of factors from which the actuary may\n\\emph{select}  link ratios include simple averages from the triangle,\naverages weighted toward more recent observations or adjusted for\noutliers, and benchmark patterns based on related, more credible loss \nexperience. Also, since the ultimate value of claims is simply the\nproduct of the most current diagonal and the cumulative product of the\nlink ratios, the completion of the interior of the triangle is usually not\ndisplayed; instead the eventual value of the claims, or ultimate value is shown.\n\nFor example, suppose the actuary decides that the volume weighted\nfactors from the claims triangle are representative of expected future\ngrowth, but discards the tail factor derived from the linear\nfit in favour of a tail based on data from a\nlarger book of similar business. The LDF method might be displayed in\nR as follows. \n<<ata>>=\nlibrary(ChainLadder) \nata(cum.triangle)\n@ \n\n\\section{Stochastic reserving models}\\label{sec:stochasticreserving}\n\nAs the provision for outstanding claims is often the biggest item on\nthe liabilities side of an insurer's balance sheet it is important \nnot only to estimate the mean but also the uncertainty of the reserve. \n\nOver the years many statistical techniques have been\ndeveloped to embed the reserving analysis into a stochastic\nframework. The key idea is to regard the observed data as one realisation of a \nrandom variable, rather than absolutes. Statistical techniques allow also \nfor more formal testing, make modelling\nassumptions more explicit and in particular help to monitor actual versus\nexpected claims developments (A vs. E). It is the regular A vs. E exercise, \nwhich can help to drive management actions. Hence, as a minimum not only the \nmean reserve, or \\emph{best estimate liabilities\\footnote{Note, actuaries \ndistinguish between best estimate liabilities un-discounted and discounted \nto present value}}, should be estimated but also the volatility of reserves, \nbased on the expected mean squared error of the reserve.\n\nIn this section we first show that the deterministic chain-ladder\nalgorithm of the previous section can be considered as a weighted linear \nregression through the origin. Indeed, the following Mack model provides \na stochastic framework for the chain-ladder method and allows us to estimate \nthe mean squared error of future payments, using many estimators from the \nlinear regression output. An alternative to the Mack model,\nis the Poisson model, a generalised linear model, that replicates the \nchain-ladder forecasts as well. Yet, the Poisson model is often not directly \napplicable to insurance data, as the variance of the data is frequently greater \nthan the mean and hence we consider a quasi-Poisson model to estimate \nuncertainty metrics. Following this we present a bootstrap technique to estimate \nthe full reserve distribution. \nFor many triangles it is reasonable to assume that the incremental payments \nfollow a log-normal distribution and hence we finish this section with a \nparametric reserving model that is particular suited to identify and model \nchanging trends in data. \n\n%% and to use statistical test to find appropriate models.\n\n\\subsection{Chain-ladder in the context of linear regression}\n\nSince the early 1990s several papers have been published to embed the\ndeterministic chain-ladder method into a statistical framework. \n\\cite{ZehnwirthBarnettProceedings, DanielMurphy1994} were not the only ones to \npoint out that the chain-ladder age-to-age link ratios could be regarded as \ncoefficients of a linear regression through the origin. To illustrate this \nconcept we follow~\\cite{ZehnwirthBarnettProceedings}. \n\nLet $C_{\\cdot,k}$ denote the $k$-th column in the cumulative claims triangle. \nThe chain-ladder algorithm can be seen as: \n\\begin{align}\n C_{\\cdot,k + 1} &  = f_k\\, C_{\\cdot, k} + \\varepsilon(k) \\mbox{ with }\n \\varepsilon_{k} \\sim N(0, \\sigma_k^2 C_{\\cdot, k}^\\delta)\n\\end{align}\nThe parameter $f_k$ describes the slope or the 'best' line through\nthe origin and data points $[C_{\\cdot,k}, C_{\\cdot, k+1}]$, with\n$\\delta$ as a 'weighting' parameter.  \\cite{ZehnwirthBarnettProceedings}\ndistinguish the cases: \n\\begin{itemize}\n\\item $\\delta = 0$ ordinary regression with intercept $0$\n\\item $\\delta = 1$ historical chain ladder age-to-age link ratios\n\\item $\\delta = 2$ straight averages of the individual link ratios\n\\end{itemize}\nIndeed, we can demonstrate the different cases by applying different\nlinear models to our data. First, we add columns to the original data\nframe \\texttt{Claims}, to have payments of the current and previous\ndevelopment period next to each other, additionally we add a column with \nthe development period as a factor.\n<<>>=\nnames(Claims)[3:4] <- c(\"inc.paid.k\", \"cum.paid.k\")\nids <- with(Claims, cbind(originf, dev))\nClaims <- within(Claims,{\n  cum.paid.kp1 <- cbind(cum.triangle[,-1], NA)[ids]\n  inc.paid.kp1 <- cbind(inc.triangle[,-1], NA)[ids]\n  devf <- factor(dev)\n  }\n)\n@ \nIn the next step we apply the linear regression function \\texttt{lm} to each \ndevelopment period, vary the weighting parameter $\\delta$ from 0 to 2 and \nextract the slope coefficients.\n<<LDFviaLM>>=\ndelta <- 0:2\nATA <- sapply(delta, function(d)\n  coef(lm(cum.paid.kp1 ~ 0 + cum.paid.k : devf, \n     weights=1/cum.paid.k^d, data=Claims))\n)\ndimnames(ATA)[[2]] <- paste(\"Delta = \", delta)\nATA\n@ \nIndeed, the development ratios for $\\delta=1$ and $\\delta=2$ tally with those \nof the previous section. Let's plot the data again, with the cumulative paid\nclaims of one period against the previous one, including the regression output\nfor each development period, see Figure~\\ref{fig:linearregression}. \n<<linerregression, include=FALSE>>=\nxyplot(cum.paid.kp1 ~ cum.paid.k | devf, \n       data=subset(Claims, dev < (n-1)), \n       main=\"Age-to-age developments\", as.table=TRUE, \n       scales=list(relation=\"free\"), \n       key=list(columns=2, lines=list(lty=1:4, type=\"l\"), \n                text=list(lab=c(\"lm(y ~ x)\",\n                                \"lm(y ~ 0 + x)\",\n                                \"lm(y ~ 0 + x, w=1/x)\",\n                                \"lm(y ~ 0 + x, w=1/x^2)\"))),\n       panel=function(x,y,...){\n         panel.xyplot(x,y,...)\n         if(length(x)>1){\n           panel.abline(lm(y ~ x), lty=1)        \n           panel.abline(lm(y ~ 0 + x), lty=2)\n           panel.abline(lm(y ~ 0 + x, weights=1/x), lty=3)\n           panel.abline(lm(y ~ 0 + x, , weights=1/x^2), lty=4)\n         }\n       }\n)\n@\nNote that for development periods 2 and 3 we observe a difference in \nthe slope of the linear regression with and without an intercept. Of course we \ncould test the significance of the intercept via the usual tests.\n\n\\begin{figure}[thb][h]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<linerregressionplot, fig=TRUE, echo=FALSE, height=5>>=\n<<linerregression>>\n@\n\\caption{Plot of the cumulative development positions from one development year to the next for each development year, including regression lines of different linear models.}\\label{fig:linearregression}\n\\end{center}\n\\end{figure}\n\n% \\begin{exercise}\n% Review the model output\n% \\end{exercise}\n\n\\subsection{The Mack model}\n\n\\cite{Mack_distributionfree1993, Mack1999} suggested a\nmodel to estimate the first two moments (mean and standard errors) of the \nchain-ladder forecast, without assuming a distribution under three \nconditions.\n\nIn order to forecast the amounts $\\hat{C}_{ik}$ for $k > n+1-i$ the Mack\nchain-ladder model assumes: \n\\begin{align}\n  \\mbox{CL1: }  & \\mathbb{E}\\left( F_{ik}| \\right.\n   \\left.  C_{i,1},C_{i,2},\\ldots,C_{i,k} \\right) = f_k\n  \\mbox{ with } F_{ik}=\\frac{C_{i,k+1}}{C_{i,k}}\\\\\n    \\mbox{CL2: } &  Var\\left( F_{i,k} | \\right.\n      \\left. C_{i,1},C_{i,2},  \\ldots,C_{ik} \\right) =\n      \\frac{\\sigma_k^2}{w_{ik} C^\\alpha_{ik}}\\\\ \n  \\mbox{CL3: } & \\left\\{ C_{i,1},\\ldots,C_{i,n}\\right\\},\n  \\left\\{ C_{j,1},\\ldots,C_{j,n}\\right\\}\\mbox{ are independent for origin\n      period } i \\neq j \n\\end{align}\nwith $w_{ik} \\in [0;1], \\alpha \\in \\{0,1,2\\}$. \nNote that Mack uses the following notation for the weighting parameter\n$\\alpha = 2 - \\delta$, with $\\delta$ defined as in the previous section.\nIn other words the Mack model assumes that the link ratios for \neach development period are consistent across all origin periods (CL1), the \nvolatility decreases as losses are paid (CL2) and all origin periods are independent, \ne.g. there is no structural change or market cycle (CL3).\nIf these assumptions hold, the Mack model gives an unbiased estimator for future \nclaims. Thus\n\\begin{align}\n  \\widehat{f}_k = \\frac{\\sum_{i=1}^{n-k} w_{ik} C^\\alpha_{i,k}F_{i,k}}\n  {\\sum_{i=1}^{n-k} w_{ik} C^\\alpha_{i,k}}\n\\end{align}\nis an unbiased estimator for $f_k$, given past observations in \nthe triangle, and  $\\widehat{f}_k$ and $\\widehat{f}_{j}$ are non-correlated \nfor $k \\neq j$.  Hence, an unbiased \nestimator for $\\mathbb{E}(C_{i,k}|C_{i,1},\\ldots,C_{i,n+1-i})$ is\n\\begin{align}\n  \\widehat{C}_{i,k} = \\widehat{f}_{n-i} \\cdot \\widehat{f}_{n-i+1} \\cdots \n  \\widehat{f}_{k-2} \\left(\\widehat{f}_{k-1}-1\\right) \\cdot C_{i,n+1-i}.\n\\end{align}\nRecall that $\\widehat{f}_k$ is the estimator with minimal variance among all \nlinear estimators obtained from the $F_{i,k}$'s. \nFinally, if $\\alpha=1$ and $\\omega_{i,k}=1$, \nthen\\footnote{for the general case see~\\cite{Mack1999}}\n\\begin{align}\n  \\widehat{\\sigma}^2_k = \\frac{1}{n-k-1}\\sum_{i=1}^{n-k} \n  \\left( F_{i,k}-\\widehat{f}_k \\right)^2 \\cdot C_{i,k}\n\\end{align}\nis an unbiased estimator of $\\sigma_k^2$, given past observations in the \ntriangle.  Based on these estimators, it is possible to compute the mean squared \nerror of prediction for reserve $\\widehat{R}_i$, given past observations \n$\\mathcal{F}$ in the triangle\n\\begin{align}\nMSE_i = \\underbrace{\\widehat{Var}(\\widehat{R}_i|\\mathcal{F})}_{\\text{process variance}} +\n \\underbrace{\\mathbb{E}\\left([R_i - \\widehat{R}_i]^2|\\mathcal{F}\\right)}_{\\text{estimation error}}.\n\\end{align}\nThe process variance originates from the stochastic movement of the process, whereas the estimation error reflects the uncertainty in the estimation of the parameters.\n\nFrom \\cite{Mack1999} (see also Chapter 3 in \\cite{WuetherichMerz:2008}), the \nprocess variance can be estimated using\n\\begin{align}\n\\widehat{Var}(\\widehat{R}_i|\\mathcal{F}) =  \\widehat{R}_{i} \\sum_{k=n+1-i}^{n-1} \n\\frac{\\widehat{\\sigma}^2_k}{\\widehat{f}_k ^2 \\widehat{C}_{i,k}},\n\\end{align}\nand the estimation error estimated by\n\\begin{align}\n\\mathbb{E}\\left([R_i - \\widehat{R}_i]^2|\\mathcal{F}\\right) = \n\\widehat{R}_i^2 \\sum_{k=n+1-i}^{n-1} \\frac{\\widehat{\\sigma}^2_{k}}{\\widehat{f}^2_{k}} \\left(\n  \\frac{1}{C_{i,k}} + \\frac{1}{\\sum_{l=1}^{n-k} C_{l,k}}\\right).\n\\end{align}\n%(where the sums are on appropriate indices, see \\cite{WuetherichMerz:2008} for \n%technical details).\n% This leads to the easily programmable recursion\n% \\begin{align}\n% \\widehat{\\mathbb{E}}\\left([C_{i,l+1} - \\widehat{C}_{i, l+1}]^2|\\mathcal{F}\\right) = \n% \\widehat{C}_{i,l}^2 \\sum_{k=l+2-i}^{l} \\frac{\\widehat{\\sigma}^2_{k}}{\\widehat{f}^2_{k}} \\left(\n%   \\frac{1}{C_{i,k}} + \\frac{1}{\\sum_{j=1}^{l+1-k} C_{j,k}}\\right) + \n%   \\widehat{\\mathbb{E}}\\left([C_{i,l} - \\widehat{C}_{i, l}]^2|\\mathcal{F}\\right)\\widehat{f}^2_l\n% \\end{align}\n% starting with $\\widehat{\\mathbb{E}}\\left([C_{i,n+1-i} - \\widehat{C}_{i, n+1-i}]^2|\\mathcal{F}\\right)=0$.\nIn order to derive the conditional mean squared error of total reserve prediction $\\widehat{R}$, define the covariance term, for $i<j$, as\n% \\begin{align}\n% MSE_{i,j}= \\widehat{R}_i \\widehat{R}_j \\left(\\sum_{k=i}^{n}\\frac{\\widehat{\\sigma}^2_{i+k}}{\\widehat{f}^2_{i+k} \\sum C_{\\cdot,k}}+\n% \\frac{\\widehat{\\sigma}^2_{j}}{[\\widehat{f}_{j-1}-1] \\widehat{f}_{j-1} \\sum C_{\\cdot,j+k}} \\right),\n% \\end{align}\n\\begin{align}\nMSE_{i,j+1} = \n \\widehat{C}_{i,j}^2 \\sum_{k=j+2-i}^{l} \\frac{\\widehat{\\sigma}^2_{k}}{\\widehat{f}^2_{k}} \\left(\n   \\frac{1}{C_{i,k}} + \\frac{1}{\\sum_{l=1}^{j+1-k} C_{l,k}}\\right) + \nMSE_{i,j},\n\\end{align}\nwith $MSE_{i, n+1-i}=0$.\n% \\begin{align}\n% MSE_{i,j}= \\widehat{R}_{i,j} \n% \\sum_{k=n+1-i}^{n-1} \\frac{\\widehat{\\sigma}^2_{k}}{\\widehat{f}^2_{k}} \\left(\n%   \\frac{1}{C_{i,k}} + \\frac{1}{\\sum_{j=1}^{n-k} C_{j,k}}\\right),\n% \\end{align}\nThen the conditional mean squared error of reserves (all years) is\n\\begin{align}\nMSE = \\sum_{i=1}^n MSE_i + 2\\sum_{j>i} MSE_{i,j}.\n\\end{align}\nThese formulas are implemented in the  \\verb@ChainLadder@ package, \\cite{chainladder}, \nvia the function \\texttt{MackChainLadder}. As an example we apply the \n\\texttt{MackChainLadder} function\\footnote{The interested reader may want \nto review the help page of the \\texttt{MackChainLadder} function in more detail and\ninvestigate the source code of the function, which utilises \\texttt{lm} and \nits output.} to our triangle:\n\n<<>>=\nlibrary(ChainLadder)\n(mack <- MackChainLadder(cum.triangle, weights=1, alpha=1, \n                        est.sigma=\"Mack\"))\n@ \nThe output provides immediate access to various statistics of the Mack model,\nincluding the forecast future payments (here labelled as IBNR) its estimated \nmean squared error, at individual and across all origin period level, \nhere $\\pm 5\\%$.\nHence, the predicted future payments and their errors can be used for an A vs. E\nexercise in the following development period, see Figure~\\ref{fig:mackplot2}, which\nwas produced using the following command.\n\n<<label=MackPlot2, include=FALSE>>=\nplot(mack, lattice=TRUE, layout=c(4,2))\n@ \n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE, height=4.5>>=\n<<MackPlot2>>\n@ \n\\caption{Plot of the actual and expected cumulative claims development and \nestimated standard error of the Mack model forecast}\\label{fig:mackplot2}\n\\end{center}\n\\end{figure}\nTo check if the assumption for the Mack model are held we review the\nresidual plots of the Mack model, see Figure~\\ref{fig:mackplot1}.\n<<label=MackPlot1, include=FALSE>>=\nplot(mack)\n@ \n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE>>=\n<<MackPlot1>>\n@ \n\\caption{Plot of \\texttt{MackChainLadder} output. The top left panel shows the \nlatest actual position with the forecasts stacked on top and whiskers indicating\nthe estimated standard error. The top right panel presents the claims developments \nto ultimate for each origin year. The four residual plots show the standardised \nresiduals against fitted values, origin, calendar and development period. \nThe residual plots should not show any obvious patterns and about 95\\% of the  \nstandardised residuals should be contained in the range of -2 to 2 for the \nMack model to be strictly applicable.}\\label{fig:mackplot1}\n\\end{center}\n\\end{figure}\nFrom the residual plots we note that smaller values appear under-fitted\nand larger values slightly over-fitted. The effect of under-fitting appears to \nbe particularly pronounced for data from earlier calendar years, where fewer data\nis available. \n%The residuals of the 2010 calendar and origin years stand out as well. \n%Hence the assumptions CL1 and  CL3 don't appear to hold and therefore the Mack \n%model might not be considered as applicable to the unadjusted data.\n\n\\begin{remark}\nOne way to address the above short-comings might be to find appropriate weights\nand to review the choice of the $\\alpha$ parameter in the Mack model. \nThe function \\texttt{CLFMdelta}, following \\cite{BardiMajidiMurphy2012}, can help \nto find consistent $\\alpha$ values based on a set of selected age-to-age ratios. \n\\end{remark}\n\n\\subsubsection{Multivariate chain-ladder models}\nThe Mack chain-ladder technique can be generalised to the multivariate\nsetting where multiple reserving triangles are modelled and developed\nsimultaneously. The advantage of the multivariate modelling is that\ncorrelations among different triangles can be modelled, which will lead\nto more accurate uncertainty assessments. \n\nReserving methods that explicitly model the between-triangle contemporaneous \ncorrelations can be found in \\cite{Prohl;Schmidt:2005, Merz;Wuthrich:2008}. \n\nAnother benefit of multivariate loss reserving is that structural\nrelationships between triangles can also be reflected, where  the\ndevelopment of one triangle depends on past losses from other\ntriangles. For example, there is generally need for the joint\ndevelopment of the paid and incurred losses, \\cite{Quarg2004}.  \n\nMost of the chain-ladder-based multivariate reserving models can be summarised\nas sequential seemingly unrelated regressions, \\cite{Zhang2010a}. We\nnote another strand of multivariate loss reserving builds a hierarchical\nstructure into the model to allow estimation of one triangle to\n\\emph{borrow strength} from other triangles, reflecting the core insight\nof actuarial credibility, \\cite{Zhang;Vanja;Guszcza:2012}.  \n\nThe \\texttt{ChainLadder} package %, \\cite{chainladder}, \nprovides implementation of multivariate chain-ladder models via the functions\n\\texttt{MunichChainLadder, MultiChainLadder} and \n\\texttt{MultiChainLadder2}. See the package vignette and help files for\nmore details.\n\n\\subsection{Poisson regression model for incremental claims}\n\n\\cite{Hachemeister-Stanard75}, ~\\cite{Kremer85} and finally \\cite{Mack91} \nexamined the validity of treating incremental paid claims as Poisson distributed random \nvariables, noting that the model gives the same forecast as the volume weighted \nchain-ladder method. \\cite{renshawverrall1998smu} presented the technique \nin the context of generalised linear models.\n\nThe idea is to assume that incremental payments $X_{ij}$ are Poisson distributed, \ngiven  the factors origin period $a_i$ and development period $b_j$ with an \nintercept $c$ and corner constraints $a_1 = 0$, $b_1 = 0$. \n\\begin{align}\n\\log \\mathbb{E}(X_{i,j}) = \\eta_{i,j} = c + a_i + b_j \n\\end{align}\n% Other paramterisation are possible, e.g. no intercept, but that makes the \n% interpretation of the test statistics less meaninfull.\nThe Poisson model can be directly implemented via the \\texttt{glm} function in R.  \n<<>>=\npreg <- glm(inc.paid.k ~ originf + devf,             \n            data=Claims, family=poisson(link = \"log\"))\nsummary(preg)\n@\nThe intercept term estimates the first log-payment of the first origin period. \nThe other coefficients are then additive to the intercept. Thus, the predictor \nfor the second payment of 2008 would be $\\exp(8.25725+0.03156-0.11739)=3538$. \nThe second column in the output above\ngives us immediate access to the standard errors. Other test statistics are provide \nby \\texttt{summary} as well, such as deviance and AIC. \n\n\nBased on those estimated coefficients we can predict the incremental claims \npayments as $\\widehat{X}_{i,j}$,\nbut first we have to create a data frame that holds the future time \nperiods.\n<<>>=\nallClaims <- data.frame(origin = sort(rep(2007:2013, n)),\n                        dev = rep(1:n,n))\nallClaims <- within(allClaims, {\n  devf <- factor(dev)\n  cal <- origin + dev  - 1\n  originf <- factor(origin)\n})\n(pred.inc.tri <- t(matrix(predict(preg,type=\"response\",\n                                  newdata=allClaims), n, n)))\n@ \nThe total amount of reserves is the sum of incremental predicted\npayments for calendar years beyond 2013\n<<>>=\nsum(predict(preg,type=\"response\", newdata=subset(allClaims, cal > 2013)))\n@ \nObserve not only the total amount of reserves is the same as the chain-ladder method, \nbut also the predicted triangle, see Remark~\\ref{re:clmultiplicative} on \npage~\\pageref{re:clmultiplicative}. \n\nFrom the regression coefficients we can also\ncalculate the chain-ladder age-to-age ratios again:\n<<>>=\ndf <- c(0, coef(preg)[(n+1):(2*n-1)])\nsapply(2:7, function(i) sum(exp(df[1:i]))/sum(exp(df[1:(i-1)])))\n@\nWhile it is interesting to assume a Poisson model, since the output is the\nsame as the one obtained using the chain-ladder technique,  we have\nto test if this model is appropriate from a statistical\nperspective. For instance, assuming equi-dispersion is clearly not valid here:\n<<>>=\nlibrary(AER)\ndispersiontest(preg)\n@ \nA quasi-Poisson model, with the variance proportional to the mean,\nshould be more reasonable. We will investigate this further in the next section.\n\\begin{remark}\nNote that the so-called Poisson regression - namely {\\em\n  \\texttt{glm(Y~..., family=poisson)}} - can be used on\nnon-integers. Recall that in {\\em {\\sffamily R}}, output from a\ngeneralised linear model is obtained using iterated least squares in a\nstandard linear regression on {\\em \\texttt{log(Y)}}, if the link\nfunction is logarithmic. The only important point in this section is\nthus to have (strictly) positive incremental payments, not integers, even this \ncan be loosened to the sum of incremental payments for each development \nperiod to be positive for a quasi-Poisson model, see~\\cite{EnglandVerrall2002} and \n\\cite{FirthRhelp2003}. \n\\end{remark}\n\n\\subsubsection{Quantifying uncertainty in GLMs}\n\nWe continue our analysis with an over-dispersion Poisson model.\nAs mentioned in \\cite{KaasGoovaertsDhaeneDenuit2008}, there are\nclosed forms for the variance of any quantity, in generalised linear\nmodels. With notations of Chapter 13, in case of the Poisson regression with a\nlogarithmic link function, we have\n\\begin{align}\n\\mathbb{E}(X_{i,j}|\\mathcal{F})  = \\mu_{i,j}  = \\exp[\\eta_{i,j}]\\text{ and }\n \\widehat{\\mu}_{i,j}  = \\exp[\\widehat{\\eta}_{i,j}].\n\\end{align}\nUsing Taylor series expansion, we can approximate  $Var(\\widehat{x}_{i,j})$\n\\begin{align}\nVar(\\widehat{x}_{i,j}) \\approx \\left|\\frac{\\partial \\mu_{i,j}}{\\partial \n\\eta_{i,j}}\\right|^2 \\cdot Var(\\widehat{\\eta}_{i,j}),\n\\end{align}\nwhich, with a logarithmic link function, can be simplified to\n\\begin{align}\n\\frac{\\partial \\mu_{i,j}}{\\partial \\eta_{i,j}} = \\mu_{i,j}.\n\\end{align}\nThus, the mean squared error of the total amount of reserve is here \n\\begin{align}\n\\mathbb{E}\\left([R-\\widehat{R}]^2\\right) \\approx \\left(\\sum_{i+j>n+1} \\widehat{\\phi} \n  \\cdot \\widehat{\\mu}_{i,j} \\right) + \\widehat{\\boldsymbol{\\mu}}' \n  \\cdot \\widehat{Var}(\\widehat{\\boldsymbol{\\eta}})  \\cdot\\widehat{\\boldsymbol{\\mu}}\n\\end{align}\nWith this preparation done we can carry out the regression, \nassuming a quasi-Poisson distribution\n<<>>=\nsummary(odpreg <- glm(inc.paid.k ~ originf + devf, data=Claims, \n                      family=quasipoisson))\n@\nNote that the coefficients are the same as for the Poisson model without \nover-dispersion. However, the dispersion parameter is \n\\Sexpr{round(summary(odpreg)$dispersion,1)} and the errors changed as well.\n%Although not all coefficient seem to be significant, it \n%doesn't mean that we should ignore them (\\textbf{WHY??}). \nNow, we can compute all the components of the mean squared error. \n<<>>=\nmu.hat <- predict(odpreg, newdata=allClaims, type=\"response\")*(allClaims$cal>2013)\nphi <- summary(odpreg)$dispersion\nSigma <- vcov(odpreg)\nmodel.formula <- as.formula(paste(\"~\", formula(odpreg)[3]))\n# Future design matrix\nX <- model.matrix(model.formula, data=allClaims)\nCov.eta <- X%*% Sigma %*%t(X)\n@\n% Why is the dispersion 22, when the dispersion test suggested 12?\nHence, the mean squared error is:\n<<>>=\nsqrt(phi * sum(mu.hat) + t(mu.hat) %*% Cov.eta %*% mu.hat)\n@\n%1708.196\nObserve that this is comparable with Mack's mean squared error of\n\\Sexpr{round(summary(mack)$Totals[\"Mack S.E.:\",])}.\n\nNevertheless, the method we just described might not be valid \nsince that expression was obtained using asymptotic theory on \ngeneralised linear models, which might not be valid here since we have \nless than 50 observations.  \n<<poissonplot, include=FALSE>>=\nop <- par(mfrow=c(2,2), oma = c(0, 0, 3, 0))\nplot(preg)\npar(op)\n@\n\\begin{verbatim}\nWarning messages:\n1: not plotting observations with leverage one:\n  7, 28 \n2: not plotting observations with leverage one:\n  7, 28\n\\end{verbatim}\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE>>=\n<<poissonplot>>\n@\n\\caption{The output of the Poisson model appears to be well behaved. \nHowever, the last plot produces a warning that should be investigated.}\n\\label{fig:poissonplot}\n\\end{center}\n\\end{figure}\nStill, the residual plots, see Figure~\\ref{fig:poissonplot}, look reasonable, \nbut \\textsf{R} gives us a warning in respect of two data points that have \na leverage  greater than one. Lines 7 and 28 refer to the development years 2 \nand 1 for the origin years 2009 and 2013 respectively. \n\nIndeed, the first payment in 2013 is considerably \nhigher than those in previous years and also the payment for the 2009 year after \n24 months is higher in relation to the payment in year 3. Again, this should\nprompt further investigations into the data.\n\n\\begin{remark}\nA wider range of generalised reserving models is provided in the \n\\texttt{ChainLadder} package via the function \\texttt{glmReserve}.\nIt takes origin period and development period as mean predictors in estimating \nthe ultimate loss reserves, and provides both analytical and bootstrapping \nmethods to compute the associated prediction errors. The bootstrapping approach \nalso generates the full predictive distribution for loss reserves. \nTo replicate the results of this section we use:\n\\end{remark}\n<<>>=\n(odp <- glmReserve(as.triangle(inc.triangle), var.power=1, cum=FALSE))\n@\n\\begin{remark}\nThe use of generalised linear models in insurance loss reserving \nhas many compelling aspects, e.g.,\n\\begin{itemize}\n \\item when the over-dispersed Poisson model is used, it reproduces the \n  estimates from the chain-ladder method\n  \\item it provides a more coherent modelling framework than the \n  Mack model\n  \\item all the relevant established statistical theory can be directly \n  applied to perform hypothesis testing and diagnostic checking\n\\end{itemize}\nHowever, the user should be cautious of some of the key assumptions \nthat underline the generalised linear model, in order to determine \nwhether this model is appropriate for the problem considered:\n\\begin{itemize}\n  \\item the generalised linear model assumes no tail development, \n  and it only projects losses to the latest time point of the observed \n  data. To use a model that enables \n  tail extrapolation, consider the growth curve model \n  \\texttt{ClarkLDF} or \\texttt{ClarkCapeCod} in the \n  \\texttt{ChainLadder} package, see also \\cite{Clark2003}\n\n  \\item the model assumes that each incremental loss is independent of \n  all the others. This assumption may not be valid in that cells from\n  the same calendar year are usually correlated due to inflation or \n  business operating factors, e.g. catastrophe losses can effect \n  policies from multiple origin periods\n\n  \\item the model tends to be over-parametrised, which may lead to \n  inferior predictive performance.\n\\end{itemize}\n\\end{remark}\n\n\\subsection{Bootstrap chain-ladder}\n\nAn alternative to asymptotic econometric relationships can be to use \nthe bootstrap methodology. Here we present a two-stage simulation approach, \nfollowing \\cite{EnglandVerrall2002}.\n\nIn the first stage a quasi-Poisson model is applied to the claims triangle \nto forecast future payments. From this we calculate the scaled-Pearson residuals, \nassuming that they are approximately independent and identical distributed.\nThese residuals are re-sampled with replacement many times \nto generate bootstrapped (pseudo) triangles and to \nforecast future claims payments to estimate the parameter error. \nRecall that the predictions of the quasi-Poisson model are the same as those\nfrom the chain-ladder method, hence we use the latter faster algorithm. \n\nIn the second stage we simulate the process error with the bootstrap value \nas the mean and an assumed process distribution, here a quasi-Poisson. \nThe set of reserves obtained in this way forms the predictive distribution, \nfrom which summary statistics such as mean, prediction error or quantiles \ncan be derived.\n\nIn a Poisson regression the Pearson's residuals are:\n\\begin{align}\n\\varepsilon_{i,j} = \\frac{Y_{i,j}-\\widehat{Y}_{i,j}}{Var(Y_{i,j})}.\n\\end{align}\nIn order to have a proper estimator of the variance (to have residuals \nwith unit variance), we have to adjust the residuals for the number of \nregression parameters $k$ (i.e. $2n - 1$) and observations $n$. \n\\begin{align}\n\\tilde{\\varepsilon}_{i,j} = \\sqrt{\\frac{n}{n-k}} \\cdot \n\\frac{Y_{i,j}-\\widehat{Y}_{i,j}}{\\sqrt{\\widehat{Y}_{i,j}}}.\n\\end{align}\nThe strategy is to bootstrap among those \nresiduals to get a sample $\\tilde{\\varepsilon}_{i,j}^b$, \nand to generate a pseudo triangle \n\\begin{align}\nY_{i,j}^b = \\widehat{Y}_{i,j} + \\sqrt{\\widehat{Y}_{i,j}} \\cdot \n\\tilde{\\varepsilon}_{i,j}^b.\n\\end{align}\n%These pseudo triangles are alternative versions of our triangle\nThen we can use standard techniques to complete the triangle, and \nextrapolate the lower part. As mentioned in the introduction to Mack's \napproach, there are two kinds of uncertainty: uncertainty in the \nestimation of the model, and uncertainty in the process of future \npayments. \n\nIf we use the predictions from a quasi-Poisson model in this new \ntriangle, we will predict the expected value of future payments. In \norder to quantify uncertainty, it is necessary to generate scenarios of \npayments. \n\n% But generating a quasi-Poisson random variable is not \n% possible. A standard strategy to generate over-dispersed payment can be \n%to use a Gamma distribution approximation, based on the first two \n%moments.\n\nThis two-stage bootstrapping/simulation approach is implemented in the\n\\texttt{BootChainLadder} function as part of the \\texttt{ChainLadder} \npackage.\n\nAs input parameters we provide the cumulative triangle, the number of \nbootstraps and the process distribution to be assumed.\n<<>>=\nset.seed(1) # set seed to have a replicatable example\n(B <- BootChainLadder(cum.triangle, R=1000, process.distr=\"od.pois\"))\n@\nThe first two moments are, not surprisingly, similar to the Poisson model. \nIn contrast to the Mack mode, which only provides the frist two moments for\nthe underlying distributions, here we have also access to the various \npercentiles of the estimated distribution of future payments. \n\nThe default plot of the model output, see Figure~\\ref{fig:BootChainLadder}, \npresents the distribution of the simulated future payments (here labelled \n\\emph{IBNR}) and an initial sense check of the model by comparing the \nlatest actual payments against simulated data.\n<<bootchainladder, include=FALSE>>=\nplot(B)\n@ \n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE>>=\n<<bootchainladder>>\n@ \n\\caption{The top left chart shows a histogram of the simulated future payments \n(here labelled \\emph{IBNR}) across all origin years. \nThe line chart in the top right presents the \nempirical cumulative distribution of those simulated future payments. \nThe bottom  row shows a breakdown by origin year. \nThe first box-whisker plot on the \nleft displays the simulations by year, with the mean highlighted as \na dot. The box-whisker plot to the right can be used to test the model, \nas it shows the latest actual claim for each origin year against the \nsimulated distribution.}\n\\label{fig:BootChainLadder}\n\\end{center}\n\\end{figure}\nFor capital setting purposes it is desirable to look at the extreme\npercentiles of the simulated data. The \\texttt{BootChainLadder} \nfunction also gives us access to the simulated triangles, which allows us to \nextract percentiles using the \\texttt{quantile} function:\n<<>>=\nquantile(B, c(0.75,0.95,0.99, 0.995))\n@ \n\nFor many lines of business in non-life insurance it is not unreasonable\nthat losses follow a log-normal distribution. We can test this idea for our \ndata by fitting a log-normal distribution to the predicted future payments.\nThe \\texttt{fitdistrplus} package by \n\\cite{Delignette-Muller2013} makes it a one liner in \\textsf{R}:\n<<>>=\nlibrary(fitdistrplus)\n(fit <- fitdist(B$IBNR.Totals[B$IBNR.Totals>0], \"lnorm\"))\n@\n<<bootchainladderfit, include=FALSE>>=\nplot(fit) \n@ \n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE>>=\n<<bootchainladderfit>>\n@\n\\caption{Plots of simulated data from \\texttt{BootChainLadder} \nand a fitted log-normal distribution.}\\label{fig:BootChainLadderFit}\n\\end{center}\n\\end{figure}\nThe fit looks very reasonable indeed, see \nFigure~\\ref{fig:BootChainLadderFit}, and the 99.5 percentile of the fitted \nlog-normal is close to the sample percentile above:\n<<>>=\nqlnorm(0.995, fit$estimate['meanlog'], fit$estimate['sdlog'])\n@\n\n\\paragraph{Payment distribution over the next year}\\label{12monthsmovment}\nAs we have access to all simulated triangles, we can also estimate \npercentiles for payments in the following year. For the 99.5 percentile \npayment over the next 12 months we get:\n<<capitalbootstrap>>=\nny <- (col(inc.triangle) == (nrow(inc.triangle) - row(inc.triangle) + 2))\npaid.ny <- apply(B$IBNR.Triangles, 3, \n                 function(x){\n                   next.year.paid <- x[col(x) == (nrow(x) - row(x) + 2)]\n                   sum(next.year.paid)\n                 })\npaid.ny.995 <- B$IBNR.Triangles[,,order(paid.ny)[round(B$R*0.995)]]\ninc.triangle.ny <- inc.triangle\n(inc.triangle.ny[ny] <- paid.ny.995[ny])\n@\nThis would reflect a \\Sexpr{round(sum(inc.triangle.ny[ny])/mean(B$IBNR.Totals)*100)}\\% \nreserve utilisation over the next year (sum of payments next year devided \nby total reserve).\n% \n% \\begin{remark}\n% See also the \\texttt{glmReserve} function of the \\texttt{ChainLadder} package, \n% which allows estimating the mean squared error via bootstrap simulations as well.\n% \\end{remark}\n\n\\subsection{Reserving based on log-incremental payments}\n\n%{\\bf Warning: from now on, develpement years start at 0}\n\nWe noted in the previous section that the claims appear to follow a\nlog-normal distribution. \\cite{Zehnwirth1994} was not the first to consider \nmodelling the log of the incremental claims payments, but his papers and software \nICRFS\\footnote{Interactive Claims Reserving and Forecasting System} \nhave popularised this  approach. Here we present the key concepts \nof what \\cite{Zehnwirth1994} calls the probabilistic trend family (PTF). \n\n% However, we shall follow the paper of \n% \\cite{Christofides1997} more closely, which is part of the Reserving Claims \n% Manual of the Institute of Actuaries.\n\nZehnwirth's model assumes the following structure for the incremental \nclaims $X_{i,j}$\n\\begin{align} \n\\ln(X_{i,j}) & = Y_{i,j} = \\alpha_i + \\sum_{k=1}^j\\gamma_k + \n\\sum_{t=1}^{i+j}\\iota_t + \\varepsilon_{i,j},\\label{Zehnwirth} \n\\end{align}\nThe errors are assumed to be normal with $\\varepsilon_{i,j} \\sim \n\\mathcal{N}(0, \\sigma^2)$. \nThe parameters $\\alpha_i, \\gamma_j, \\iota_t$ model trends in \nthree time directions, namely origin year, development year and \ncalendar (or payment) year respectively, see Figure~\\ref{fig:triangleStructure}. \n%Suppose claims payments would decrease by 50\\% every development year,\n% \\begin{figure}[thb]\n% \\begin{center}\n% \\includegraphics{figures/triangle.png}\n% \\end{center}\n% \\end{figure}\n\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.3\\textwidth}\n<<trianglestructure, fig=TRUE, echo=FALSE, width=4, height=4>>=\nop <- par(mar = rep(0, 4))\nplot.new()\nplot.window(xlim=c(0,1), ylim=c(0,1), asp=1, main=\"h\")\narrows(x0=0,y0=1, x1=0, y1=0)\ntext(x=0.03, y=0.5, label=\"origin period\", srt=90)\narrows(x0=0,y0=1, x1=1, y1=1)\ntext(x=0.5, y=0.97, label=\"development period\")\narrows(x0=0,y0=1, x1=0.5, y1=0.5)\ntext(x=0.33, y=0.73, label=\"calendar period\", srt=-45)\npar(op)\n@\n\\caption{Structure of a typical claims triangle and the three time directions: \norigin, development and calendar periods.}\n\\label{fig:triangleStructure}\n\\end{center}\n\\end{figure}\n\n\\cite{Christofides1997} examines a very similar model, but uses the following \nnotation\n\\begin{align} \n\\ln(X_{i,j}) & = Y_{i,j} = a_i + d_j + \\varepsilon_{i,j},\n\\label{Christofides}\n\\end{align}\nwith $a$, $d$ representing the parameters in origin and development period \ndirection (a parameter $p_{i+j-1}$ for the payment year direction \ncould be added). Although models \\ref{Zehnwirth} and \\ref{Christofides} \nare essentially the same, the design matrices differ and \ntherefore the coefficients and their interpretation.\n\\begin{remark}\nNote that the above model is not a GLM, e.g. \n$\\log(y+\\varepsilon)=X\\beta$. Instead it models $\\log(y)=X\\beta+\\varepsilon$; \nalthough both models assume $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\nHence, we will use least square regression to fit the coefficients\nvia \\texttt{lm} again.\n\\end{remark}\nBefore we apply the log-linear model to the data, and we will follow \n\\cite{Christofides1997}, we shall plot it again on a log scale.\n<<>>=\nClaims <- within(Claims, {\n    log.inc <- log(inc.paid.k)\n    cal <- as.numeric(levels(originf))[originf] + dev - 1\n})\n@\n<<loginc,include=FALSE, echo=FALSE>>=\nwith(Claims,{ \n  interaction.plot(x.factor=dev, trace.factor=originf, response=log.inc, \n                   fun=sum, type=\"b\", bty='n'); axis(1, at=1:n)\n  title(\"Incremental log claims development\")\n})\n@\nThe interaction plot, Figure~\\ref{fig:adjusteddata}, suggests a linear \nrelationship  after the second development year on a log-scale. The \nlines of the different origin years are fairly closely group, but the \nlast two years, labelled 6 and 7, do stand out. \nWe shall test if this is significant.\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<echo=FALSE, fig=TRUE, height=4>>=\n<<loginc>>\n@\n\\caption{The interaction plot shows the developments of the origin years on a \nlog scale. From the second development year the decay appears to be linear.\n}\\label{fig:adjusteddata}\n\\end{center}\n\\end{figure}\nWe start with a model using all levels of the origin factor \nand two dummy parameters for the development year, with $d_1=d1$ and\n$d_j = (j - 1) \\cdot d27$ for $j>1$.  \nHence, we add two dummy variables to \nour data.\n<<>>=\nClaims <- within(Claims, { \n  d1 <- ifelse(dev < 2, 1, 0)\n  d27 <- ifelse(dev < 2, 0, dev - 1)\n})        \n@ \nThe dummy variable $d1$ is $1$ for the first development period and $0$ \notherwise, while $d27$ is $0$ for the first development period and counts \nup from $1$ then onwards. Hence, we will estimate one parameter for the first \npayment and a constant trend (decay) for the following periods.\n% o2012 <- ifelse(origin == 2012, 1, 0)\n% o2013 <- ifelse(origin == 2013, 1, 0)\n<<>>=\nsummary(fit1 <- lm(log.inc ~  originf + d1 + d27, data=Claims))\n@ \nThe model output confirms what we had noticed from the interaction\nplot already, apart from the origin years 2012 and 2013 there is no \nsignificant difference between the years; the p-values are all greater than 5\\% and\nthe coefficients are less than twice their standard errors. \nTherefore we reduce the model and replace the origin variable with two dummy \ncolumns for those years.\n% ff=Claims$origin\n% levels(ff)=c(rep('base',5), '2012', 2013)\n% summary(Fit2 <- lm(log.inc ~ ff + d1 + d27, data=Claims))\n<<>>=\nClaims <- within(Claims, { \n  a6 <- ifelse(originf == 2012, 1, 0)\n  a7 <- ifelse(originf == 2013, 1, 0)\n})\nsummary(fit2 <- lm(log.inc ~  a6 + a7 + d1 + d27, data=Claims))\n@\n% summary(fit2 <- update(fit1, ~ . + a6 + a7 - origin, data=Claims))\nThe reduction in parameters from 9 to 5 seems sensible, all coefficient \nare significant and the model error reduced from \n\\Sexpr{round(summary(fit1)$sigma,3)} to \\Sexpr{round(summary(fit2)$sigma,3)} as \nwell.  Further we can read off the coefficient for $d27$ that claims \npayments are predicted to reduce by 44\\% each year after year one.\nNext, we plot the model:\n<<Fit2plot2, include=FALSE>>=\nop <- par(mfrow=c(2,2), oma = c(0, 0, 3, 0))\nplot(fit2)\npar(op)\n@\nReviewing the residual plots in Figure~\\ref{fig:fit2plot2} highlights \nagain the latest payment for the 2009 origin year (the 18th row of the \nClaims data) as a potential outlier.\n\nThe error distribution  appears to follow a normal distribution, top right \nqq-plot in Figure~\\ref{fig:fit2plot2}, confirmed by the Shapiro-Wilk normality test. \n<<>>=\nshapiro.test(fit2$residuals)\n@\n\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<echo=FALSE, fig=TRUE>>=\n<<Fit2plot2>>\n@\n\\caption{Residual plots of the log-incremental model \\texttt{fit2}.\nThe last payment of 2009 (row 18) is highlighted again as a potential outlier, \nso are rows 11, 7 and 4.}\\label{fig:fit2plot2}\n\\end{center}\n\\end{figure}\n% We can test as well if the resdiuals are distributed normally.\n% <<>>=\n% shapiro.test(fit2$residuals)\n% @\n% The Shapiro-Wilk test doesn't reject the null hypothesis that the \n% residuals are normally distributed. \nTo investigate the residuals further we shall plot them against the \nfitted values and the three trend directions. \nThe following function will create those four plots for our \nmodel.\n<<>>=\nresPlot <- function(model, data){\n  xvals <- list(\n    fitted = model[['fitted.values']],\n    origin = as.numeric(levels(data$originf))[data$originf],\n    cal=data$cal, dev=data$dev\n  )\n  op <- par(mfrow=c(2,2), oma = c(0, 0, 3, 0))\n  for(i in 1:4){\n  plot.default(rstandard(model) ~ xvals[[i]] ,\n               main=paste(\"Residuals vs\", names(xvals)[i] ),\n               xlab=names(xvals)[i], ylab=\"Standardized residuals\")\n  panel.smooth(y=rstandard(model), x=xvals[[i]])\n  abline(h=0, lty=2)\n  }\n  mtext(as.character(model$call)[2], outer = TRUE, cex = 1.2)\n  par(op)\n}\n@\n<<Fit2plot, include=FALSE>>=\nresPlot(fit2, Claims)\n@\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<echo=FALSE, fig=TRUE>>=\n<<Fit2plot>>\n@\n\\caption{Residual plots of the log-incremental model \\texttt{fit2} against \nfitted values and the three trend directions.}\\label{fig:fit2plot}\n\\end{center}\n\\end{figure}\nAgain, the residual plots all look fairly well behaved, \nhowever, we notice from the bottom left plot in Figure~\\ref{fig:fit2plot} \nthat claims for the payment years 2007, 2008 are slightly over-fitted and \n2009, 2010 are under-fitted. Hence, we introduce an additional parameter \nfor that period and update our model.\n<<>>=\nClaims <- within(Claims, { \n  p34 <- ifelse(cal < 2011 & cal > 2008, cal-2008, 0)\n})\nsummary(fit3 <- update(fit2, ~  . + p34, data=Claims))\n@\n<<Fit3plot, include=FALSE>>=\nresPlot(fit3, Claims)\n@\nThe residual plot against calendar years, Figure~\\ref{fig:fit3plot}, has \nimproved and the parameter $p34$ could be regarded significant.\nThe coefficient $p34$ describes a 6\\% increase of claims payments in those two \nyears. An investigation should clarify if this effect is the result of a \ntemporary increase in claims  inflation, a change in the claims settling \nprocess, other causes or just random noise.\nObserve that the new model has a slightly lower residual standard error of \n\\Sexpr{round(summary(fit3)$sigma,3)} compared to \\Sexpr{round(summary(fit2)$sigma,3)}. \n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<echo=FALSE, fig=TRUE>>=\n<<Fit3plot>>\n@\n\\caption{Residual plot of the log-incremental model \\texttt{fit3}.}\\label{fig:fit3plot}\n\\end{center}\n\\end{figure}\n% Test the model by taking the latest calendar year data out\n% <<>>=\n% Fit3_2012 <- update(Fit3, data=subset(Claims, cal <= 2012))\n% testCoef <- cbind(summary(Fit3)$coefficients[,1:2], \n%                   summary(Fit3_2012)$coefficients[,1:2])\n% colnames(testCoef)[c(1,3)] <- paste(\"Up to\", 2012:2013)\n% testCoef\n% @\n\nWithin the linear regression framework we can forecast the claims\npayments and estimated the standard errors. We follow the paper by \n\\cite{Christofides1997} again. Recall that for a log-normal distribution the mean\nis $E(X) = \\exp(\\mu + 1/2 \\sigma^2)$ and the variance is\n$Var(X) = \\exp(2\\mu + \\sigma^2)(\\exp(\\sigma^2) - 1)$,\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of the logarithm.\n<<>>=\nlog.incr.predict <- function(model, newdata){\n  Pred <- predict(model, newdata=newdata, se.fit=TRUE)  \n  Y <- Pred$fit\n  VarY <- Pred$se.fit^2 + Pred$residual.scale^2 \n  P <- exp(Y + VarY/2)\n  VarP <-  P^2*(exp(VarY)-1)\n  seP <- sqrt(VarP)\n  model.formula <- as.formula(paste(\"~\", formula(model)[3]))\n  mframe <- model.frame(model.formula, data=newdata)\n  X <- model.matrix(model.formula, data=newdata)\n  varcovar <- X %*% vcov(model) %*% t(X)\n  CoVar <-  sweep(sweep((exp(varcovar)-1), 1, P, \"*\"), 2, P, \"*\")\n  CoVar[col(CoVar)==row(CoVar)] <- 0 \n  Total.SE <- sqrt(sum(CoVar) + sum(VarP))\n  Total.Reserve <- sum(P)\n  Incr=data.frame(newdata, Y, VarY, P, seP, CV=seP/P)\n  out <- list(Forecast=Incr,              \n              Totals=data.frame(Total.Reserve, \n                                Total.SE=Total.SE, \n                                CV=Total.SE/Total.Reserve))\n  return(out)\n}\n@ \n% Z <- t(P) %*% (exp(varcovar)) %*% P\n% Total.SE <- sqrt(Z + sum(VarP))\nWith the above function it is straightforward to carry out the\nprediction for future claims payment and standard errors. \nAs a bonus we can estimate payments beyond the available data.\n\nTo forecast the future claims we prepare a data frame with the \npredictors for those years, here with 6 years beyond age 7. \n<<>>=\ntail.years <-6\nfdat <- data.frame(\n  origin=rep(2007:2013, n+tail.years),\n  dev=rep(1:(n+tail.years), each=n)\n  )\nfdat <- within(fdat, {\n  cal <- origin + dev - 1\n  a7 <- ifelse(origin == 2013, 1, 0)\n  a6 <- ifelse(origin == 2012, 1, 0)\n  originf <- factor(origin)\n  p34 <- ifelse(cal < 2011 & cal > 2008, cal-2008, 0)\n  d1 <- ifelse(dev < 2, 1, 0)\n  d27 <- ifelse(dev < 2, 0, dev - 1)\n})  \n@ \nSo, here are the results for the two models:\n<<>>=\nreserve2 <- log.incr.predict(fit2, subset(fdat, cal>2013))\nreserve2$Totals \nreserve3 <- log.incr.predict(fit3, subset(fdat, cal>2013))\nreserve3$Totals\n@ \nThe two models produce very similar results and it shouldn't be much\nof a surprise as they are quite similar indeed. The third model has \nproportionally a slightly smaller standard error and may hence \nbe the preferred choice.  \n\nThe future payments can be displayed with the \\texttt{xtabs} function:\n<<>>=\nround(xtabs(P ~ origin + dev, reserve3$Forecast))\n@\nThe model structure is clearly visible in the above future claims triangle; as \nthe origin years 2007 to 2011 share the same parameter, the predicted future \npayments for those years have the same identical mean expectations.\n\nFor comparison here is the output of the Mack chain-ladder model, assuming a\ntail factor of 1.05 and standard error of 0.02: \n<<>>=\nround(summary(MackChainLadder(cum.triangle, est.sigma=\"Mack\",\n                     tail=1.05, tail.se=0.02))$Totals,2)\n@ \nThe chain ladder method provides similar forecast to the\nlog-incremental regression model, but at the price of many more\nparameters and hence potential instability. \n\nA model with few parameters is potentially more robust and can be analysed by\nback testing the model with fewer data points.\n\nThe log-incremental regression model provides an intuitive \nand elegant stochastic claims reserving model and can help to investigate \ntrends in the calendar/payment year direction, such as claims inflation, which is\nchallenging to define and measure, \\cite{GesmannRayeesClapham2013}. \n%Those trend changes can highlight movements in claims inflation, \n%or indeed changes in the claims settling process. \n%Neither of those factors should be ignored. \nAdditionally the tail extrapolation is part of the model design and not a \nartificial add on. \n\nSee \\cite{Christofides1997} and \\cite{Zehnwirth1994} for a more detailed \ndiscussion of the log-incremental model.\n\n\\section{Quantifying reserve risk}\\label{sec:reserverisk}\n\nThe most frequent reason insurance companies failed in the past were\ninsufficient reserves, \\cite{Massey2002}. \nHence, as mentioned earlier, monitoring claims development is one of main \npurposes of the models presented in the previous section. Models, unlike \ndeterministic point estimators, allow the actuary to judge the materiality of \npayment deviation from modelled expected claims development.\n\nYet, we have to acknowledge that a model cannot be proven right, or, to put it\nmore bluntly: \\emph{You make money, until you don't.}\n\nTherefore it is important to understand the underlying model assumptions\nand to quantify the uncertainty of the predicted mean ultimate loss costs.\nTypical risk measures are mean squared error, value at risk (VaR) and \ntail value at risk (TVaR). These risk metrics can be\ndefined over different time horizons and play a key role in assessing\ncapital requirements for reserve risk.\n\n\\subsection{Ultimo reserve risk}\n\nThe uncertainty around the ultimate loss cost, also called \\emph{ultimo} reserve \nrisk, estimates the risk that the reserve is not sufficient to cover claims \npayments for the full run-off of today's liabilities. It is an important metric \nfor capital setting and when pricing the transfer of run-off books of business and \nhas been in use for many years.\n\nEstimators for the ultimo risk were given for all the stochastic reserving \nmodels of the previous section. The log-incremental and bootstrap models\nprovide direct access to various percentiles, while the Mack model only provides\nestimation for the mean and mean squared error and hence requires \na distribution assumption, e.g. log-normal to estimate extreme percentiles.\n\n\\subsection{One-year reserve risk}\n\nThe reserve risk over a one-year time horizon measures the change required in \nthe estimate ultimate loss cost conditioned on the claims development over \nthe following year. This metric is used to assess the reserve risk under \nthe proposed Solvency II regime.\n\nThe Solvency II framework is consistent with a marked to market basis, and \ntherefore focuses on the one-year view of the balance sheet, which requires \nthe discounting of future cash flows based on an assumed interest rate(s).\nReserve risk is specifically is defined as the difference between the best \nestimate reserve at $t=0$ and after 12 months ($t=1$), following claims \ndeterioration or a claims shock with $0.5\\%$ probability.\n\n\\cite{merzwuthrich2008mcd} analysed the Mack model and derived analytical \nformulas for the \\emph{claims development result (CDR)}.\n\\cite{ohlsson2009one} also describe a simulation approach to quantify the one-year \nrisk.\n\nEstimating the one-year reserve risk can be computational demanding and many \naspects such as the inclusion of expert judgement and tail factors are fields of\nactive research.\n\n\\begin{remark}\nSimilar ideas to the one-year reserve risk are applied to back test reserving \nmodels. By removing the latest calendar year data from a claims triangle and \ncomparing its prediction with the forecast based on the complete \ndata we can test the robustness of the model and its parameters.\n\\end{remark}\n\n\\paragraph{Illustrative one-year reserve risk example}\n\nTo clarify the concept, we present an illustrative example of how to estimate \nthe one-year reserve risk. We follow the ideas of \\cite{FeliskyAkohCabrera2010} and \nsimplify those even further.\n\nFrom our bootstrap model we extracted the future claims triangle that has shown \nthe  highest payment in the following calendar year at the 99.5 percentile, see\npage~\\pageref{12monthsmovment}. \nWe add the shock payment of the next year to the original \ntriangle and re-forecast the extended triangle to ultimate, using the \nchain-ladder algorithm (note that the age-to-age link ratios will change,\nbut no tail factor is assumed), \nand compare the newly predicted reserve with the original forecast.\n<<>>=\n(cum.triangle.ny <- t(apply(inc.triangle.ny,1,cumsum)))\nf.ny <- sapply((n-1):1, function(i){\n  sum(cum.triangle.ny[1:(i+1), n-i+1])/sum(cum.triangle.ny[1:(i+1), n-i])\n})\n(f.ny <- c(f.ny[-(n-1)],1))\nfull.triangle.ny <- cum.triangle.ny\nfor(k in 2:(n-1)){ \n  full.triangle.ny[(n-k+2):n, k+1] <- full.triangle.ny[(n-k+2):n,k]*f[k]\n}\n(sum(re.reserve.995 <- full.triangle.ny[,n] - rev(latest.paid)))\n@\nWe observe that after a 1 in 200 payment shock year the reserve would increase \nfrom \\Sexpr{format(round(sum(reserve)), big.mark=\",\")}\nto \\Sexpr{format(round(sum(re.reserve.995)), big.mark=\",\")} \n($+$\\Sexpr{round(sum(re.reserve.995)/sum(reserve)*100-100)}\\%) Therefore the\none-year reserve risk is\n\\Sexpr{format(round(sum(re.reserve.995)-sum(reserve), 1), big.mark=\",\")},\nassuming that the actuary would not change further assumptions. \n\nTo put this metric into context, suppose the reserve is twice \nthe volume of net earned premiums in the following year, then this would suggest \na prior year deterioration \nof \\Sexpr{2*round((sum(re.reserve.995)/sum(reserve)*100-100))}\\%\non the combined ratio. \n\nThis simplified re-reserving approach has of course\nits limitation, it is only a point estimator, the payment shock movement is purely \ndependent on the historical data volatility and cannot capture \nevents such as changes in legislation or other exogenous influences. \n\nNote that the reserve after the one year shock development is \nless than the predictions that includ a tail factor. This demonstrates \nthat estimating the one-year reserve risk is far \nmore complex than illustrated with the toy example above and \nthat the actuary would have to consider carefully how she would re-reserve the\nsame book of business in the following year, e.g. she may want to re-consider\ntrends in the calendar year direction.\n\nIn reality, if we had a massive spike in paid, unless this related to a single \nclaim, which seems unlikely at 1 in 200 level, chances are we have issues in \nmany areas.  Therefore we will probably cross check claims with material case \nreserves and review them as well. The influence of case reserves is ignored in \nthis example.\n\n\\begin{remark}\nLong tail classes of business may have a lower reserve risk over a one-year \nhorizon than short tail lines of business. Their claims development can be much \nslower and hence changes can take longer to materialise. \n\nOn the other hand it is also more difficult to detect reserve \ndeterioration for long tail lines. Exemplified in the past soft US-casualty cycle \nof the late 1990's that showed prior year deteriorations reported over several \nyears.\n\\end{remark}\n\n\\section{Discussion}\n\nThis chapter gave a brief overview of how some of the more popular claims \nreserving methods and models can be applied in \\textsf{R}. For more details \nabout the models and their mathematical derivation see the original papers, \n\\cite{WuetherichMerz:2008} or  \\cite{KaasGoovaertsDhaeneDenuit2008}.\n\nClaims reserving is a complex and evolving subject. Changes in regulation, \ne.g. Solvency II in Europe, are expected to accelerate the adoption of stochastic \nreserving frameworks. Although, the Mack and Bootstrap model, which are \nsimple to implement in spreadsheet software, are popular\nwith actuaries today, %\\cite{JamesOrr:2012}.\nwe demonstrated that many other stochastic models are straightforward to \napply in a statistical environment such as \\textsf{R}.\n\nThere are many other aspects that need to be considered in a full \nreserving analysis and which were not covered here; to name but a few:\n\\begin{itemize}\n\\item claims inflation and other exogenous influences such as legislation, \n  social environment, climate change%, see \\cite{GesmannRayeesClapham2013}\n\\item changes in business processes, earning patterns, accounting practice \n  and data quality in general\n\\item changes to exposure and underlying policy terms \\& conditions \n\\item aggregation of reserves across multiple lines of business\n\\item treatment of large and catastrophe losses\n\\item treatment of reinsurance\n\\end{itemize}\n%Looking into the future, it is conceivable that actuaries will use Bayesian \n%models based on individual claims data, rather than aggregated triangles, as it\n%is often the case today. \n\nReserving is always mixture of art and science, and requires a combination of \nsound data  analysis with business knowledge and judgement. \nIn this sense a Bayesian approach would lead naturally to the inclusion of \nexpert judgement and  the full reserve distribution.\n\nUsing hierarchical or multilevel models as presented by \\cite{Guszcza2008} \nappears a natural next step for claims reserving.  \nThe Clark LDF model, \\cite{Clark2003}, mentioned by \\cite{Guszcza2008}, has already \nbeen implemented by Daniel Murphy in \\textsf{R} as part of the \\texttt{ChainLadder} \npackage. Additionally the double-chain-ladder approach of \n\\cite{MariaDoloresMartinezMiranda2011} looks like a promising extension of the \nchain-ladder family of models.\n\nAlso worth mentioning is the \\texttt{lossDev} package by \n\\cite{LawsSchmid2011} that implements robust loss development using \nMarkov Chain Monte Carlo (MCMC) using \\texttt{rjags}, \\cite{rjags}.\n\n\\paragraph{So, which reserving model should I use?}\nIt depends. Unfortunately, there is no easy answer. \n\nIt depends on the data, the context, the type of business, the tail \ncharacteristic, the time available, your statistical knowledge and of course \nthe aim of the analysis. \n\nRemember the purpose of the data is to prove the model wrong. Hence, it is often\neasier to start by reviewing, which models not to consider.\n\n\\section{Acknowledgement}\nThe author would like to thank his co-authors of the \\texttt{ChainLadder} \npackage Daniel Murphy and Yanwei Zhang for their contribution to the project\nand without whom this text wouldn't have been possible. Additionally, \nDavid Menezes for his invaluable feedback on a draft version of this chapter.\nFinally Arthur Charpentier, who, as the editor of this book, has provided much \nguidance and encouragement along the way.  Of course any errors and mistakes \nare the author's own responsibility.\n\n\\section{Exercises}\n\n\\begin{exercise}\n\\cite{Christofides1997} provides data on exposure changes and inflation for the \nexample triangle used in this chapter:\n<<>>=\nexposure <- data.frame(origin=factor(2007:2013),\n  volume.index=c(1.43, 1.45, 1.52, 1.35, 1.29, 1.47, 1.91))\ninflation <- data.frame(cal=2007:2013, \n  earning.index=c(1.55, 1.41, 1.3, 1.23, 1.13, 1.05, 1))\n@\nAdjust the historical data for those exposure changes and inflationary effects,\nusing\n\\begin{align}\n\\mbox{Normalised claims} &= \\frac{\\mbox{Claims} \\times \\mbox{Inflation}}\n                                {\\mbox{Exposure}}\n                                \\label{eq:ClaimsAdjustment}\n\\end{align}\nCarry out a reserving analysis. Which changes do you observe?\nHow do you scale the output back to the original scale?\n\\end{exercise}\n\n\\begin{exercise}\nConsider different scenarios of future claims inflation. \nBy how much does the reserve change if you set claims inflation at 7.5\\%?\n\\end{exercise}\n\n\\begin{exercise}\nAdd two new arguments to the function \\texttt{log.incr.predict} to take into \naccount exposure and inflation assumptions.\n\\end{exercise}\n\n\\begin{exercise}\nTest the stability of your reserve. Follow the approach by\n\\cite{ZehnwirthBarnettProceedings}, remove the latest calendar year information\nfrom your data and re-forecast the reserve. Re-fit your model and discuss if \nthe changes in the parameters are significant.\n\\end{exercise}\n\n\\begin{exercise}\nThe \\texttt{ABC} triangle of the \\texttt{ChainLadder} package shows \nsignificant calendar year trends, see Figure \\ref{fig:MackABC}.\n<<>>=\nlibrary(ChainLadder)\ndata(ABC)\nM <- MackChainLadder(ABC/1000, est.sigma=\"Mack\")\n@\n<<ABC, include=FALSE>>=\nplot(M)\n@\nInvestigate the data with the log-incremental model. Can calendar year \ntrend parameter be found?\n\\begin{figure}[thb]\n\\begin{center}\n\\setkeys{Gin}{width=0.75\\textwidth}\n<<fig=TRUE, echo=FALSE>>=\n<<ABC>>\n@\n\\caption{Mack chain-ladder output for the \\texttt{ABC} triangle}\\label{fig:MackABC}\n\\end{center}\n\\end{figure}\n\\end{exercise}\n\n\\begin{exercise}\n\nThe log-incremental model estimates the first two moments of the reserve, \nassuming the incremental claims data follow a log-normal distribution.\nEstimate the 99.5 percentile movement of the reserve  \nand compare against the output of the bootstrap chain-ladder model,\n\\end{exercise}\n\n% \n% \\begin{exercise}\n% Review the help file of \\texttt{MultiChainLadder}, which allows to model the \n% reserve of several triangles at the same time. How is it related to the Munich\n% \\end{exercise}\n\n\\begin{exercise}\nThe \\texttt{glm} function has the argument \\texttt{offset}. Discuss how can it \nbe used to take into account exposure information. See also the \\texttt{glmReserve} \nfunction in the \\texttt{ChainLadder} function.\n\\end{exercise}\n\n\n\\begin{exercise}\nGlenn Meyers and Peng Shi provide loss reserving data pulled from NAIC \nSchedule P via the CAS web site:\n\\url{http://www.casact.org/research/index.cfm?fa=loss_reserves_data}.\nRead the data into \\textsf{R} and analyse it. \n\nInvestigate how multivariate chain-ladder models can be used to estimate the \nreserve for several of these triangles simultaneously.\n\\end{exercise}\n\n\\bibliographystyle{plainnat}\n\\bibliography{Chapter_15}\n\n%\\clearpage\n%\\printindex\n\n\n\\end{document}",
    "created" : 1380355765184.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2173291407",
    "id" : "1EFF9F4F",
    "lastKnownWriteTime" : 1381089854,
    "path" : "~/Dropbox/caswr/LaTeX/Chapters/chapter15/Chapter_15.Rnw",
    "project_path" : null,
    "properties" : {
        "ignored_words" : "eps,ChainLadder,un,bn,ec,europa,eu,htm,stats,i,j,ik,ij,vs,th,lm,Var,w,MackChainLadder,CLFMdelta,MunichChainLadder,MultiChainLadder,glm,poisson,GLMs,odpreg,glmReserve,ClarkLDF,ClarkCapeCod,BootChainLadder,fitdistrplus,inc,ny,Zehnwirth's,Wilk,xtabs,TVaR,lossDev,rjags,Yanwei,Charpentier,incr,equi,Peng,Shi,www,casact,org,cfm,plainnat,Menezes\n"
    },
    "source_on_save" : false,
    "type" : "sweave"
}